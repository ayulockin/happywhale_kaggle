{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ca1f2c-0027-425f-9471-5b5a7e1c64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847cacd",
   "metadata": {},
   "source": [
    "# Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521d590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import gc\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from model import SimpleSupervisedtModel, ArcFaceSupervisedModel, get_feature_extractor\n",
    "from config import get_train_config\n",
    "from data import GetDataloader\n",
    "from utils import ShowBatch, id_generator\n",
    "from callbacks import GetCallbacks\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e29bec0-a28b-40be-93ab-0215aec0cc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 256,\n",
      " 'early_patience': 6,\n",
      " 'embedding_save_path': '../embeddings',\n",
      " 'epochs': 30,\n",
      " 'exp_id': 'VYDM715W',\n",
      " 'image_height': 128,\n",
      " 'image_width': 128,\n",
      " 'labels': {'beluga': 4,\n",
      "            'blue_whale': 7,\n",
      "            'bottlenose_dolphin': 3,\n",
      "            'brydes_whale': 19,\n",
      "            'commersons_dolphin': 20,\n",
      "            'common_dolphin': 10,\n",
      "            'cuviers_beaked_whale': 17,\n",
      "            'dusky_dolphin': 13,\n",
      "            'false_killer_whale': 2,\n",
      "            'fin_whale': 6,\n",
      "            'frasiers_dolphin': 25,\n",
      "            'gray_whale': 8,\n",
      "            'humpback_whale': 1,\n",
      "            'killer_whale': 11,\n",
      "            'long_finned_pilot_whale': 14,\n",
      "            'melon_headed_whale': 0,\n",
      "            'minke_whale': 5,\n",
      "            'pantropic_spotted_dolphin': 23,\n",
      "            'pygmy_killer_whale': 24,\n",
      "            'rough_toothed_dolphin': 22,\n",
      "            'sei_whale': 15,\n",
      "            'short_finned_pilot_whale': 12,\n",
      "            'southern_right_whale': 9,\n",
      "            'spinner_dolphin': 16,\n",
      "            'spotted_dolphin': 18,\n",
      "            'white_sided_dolphin': 21},\n",
      " 'model_save_path': '../models',\n",
      " 'num_folds': 5,\n",
      " 'num_labels': 26,\n",
      " 'resize': False,\n",
      " 'rlrp_factor': 0.2,\n",
      " 'rlrp_patience': 3,\n",
      " 'train_img_path': '../128x128/train_images-128-128/train_images-128-128',\n",
      " 'use_arcface': False,\n",
      " 'use_augmentations': True}\n"
     ]
    }
   ],
   "source": [
    "args = get_train_config()\n",
    "\n",
    "random_id = id_generator(size=8)\n",
    "args.exp_id = random_id\n",
    "\n",
    "pp.pprint(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d587d3b-0e72-4361-8ac4-01b787aaa69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd34cdd",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7356d565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Labels: 26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species individual_id  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9   \n",
       "1  000562241d384d.jpg      humpback_whale  1a71fbb72250   \n",
       "2  0007c33415ce37.jpg  false_killer_whale  60008f293a2b   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin  4b00fe572063   \n",
       "4  00087baf5cef7a.jpg      humpback_whale  8e5253662392   \n",
       "\n",
       "                                            img_path  target  fold  \n",
       "0  ../128x128/train_images-128-128/train_images-1...       0   2.0  \n",
       "1  ../128x128/train_images-128-128/train_images-1...       1   3.0  \n",
       "2  ../128x128/train_images-128-128/train_images-1...       2   2.0  \n",
       "3  ../128x128/train_images-128-128/train_images-1...       3   2.0  \n",
       "4  ../128x128/train_images-128-128/train_images-1...       1   4.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../cleaned_5_fold_train.csv')\n",
    "print('Num Labels:', args.num_labels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1baab6e",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d75a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "if DEBUG:\n",
    "    # Get Split\n",
    "    train_df = df[df.fold != 0]\n",
    "    valid_df = df[df.fold == 0]\n",
    "\n",
    "    # Get train and validation loaders\n",
    "    dataset = GetDataloader(args)\n",
    "    trainloader = dataset.dataloader(train_df, data_type='train')\n",
    "    validloader = dataset.dataloader(valid_df, data_type='valid')\n",
    "\n",
    "    # Display a batch\n",
    "    if args.use_arcface:\n",
    "        sample_inputs, sample_labels = next(iter(trainloader))\n",
    "        sample_imgs, sample_labels = sample_inputs['img_input'], sample_inputs['label_input']\n",
    "    else:\n",
    "        sample_imgs, sample_labels = next(iter(trainloader))\n",
    "\n",
    "    show_batch = ShowBatch(args)\n",
    "    show_batch.show_batch(sample_imgs, sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b6305",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b39728",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    if args.use_arcface:\n",
    "        get_model = ArcFaceSupervisedModel(args)\n",
    "    else:\n",
    "        get_model = SimpleSupervisedtModel(args)\n",
    "        \n",
    "    model = get_model.get_efficientnet()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20adc528",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "469dc68a-0f3f-4c52-af75-c39924032f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = GetCallbacks(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2cc61",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99725906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayut\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ayut/happywhale/runs/19qx9i6i\" target=\"_blank\">VYDM715W_0_train</a></strong> to <a href=\"https://wandb.ai/ayut/happywhale\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "160/160 [==============================] - 75s 397ms/step - loss: 0.8150 - acc: 0.7590 - top@1_acc: 0.7590 - top@5_acc: 0.9416 - val_loss: 0.4165 - val_acc: 0.8711 - val_top@1_acc: 0.8711 - val_top@5_acc: 0.9869\n",
      "Epoch 2/30\n",
      "160/160 [==============================] - 58s 358ms/step - loss: 0.3413 - acc: 0.8905 - top@1_acc: 0.8905 - top@5_acc: 0.9920 - val_loss: 0.2885 - val_acc: 0.9109 - val_top@1_acc: 0.9109 - val_top@5_acc: 0.9930\n",
      "Epoch 3/30\n",
      "160/160 [==============================] - 58s 361ms/step - loss: 0.2498 - acc: 0.9190 - top@1_acc: 0.9190 - top@5_acc: 0.9956 - val_loss: 0.2664 - val_acc: 0.9186 - val_top@1_acc: 0.9186 - val_top@5_acc: 0.9931\n",
      "Epoch 4/30\n",
      "160/160 [==============================] - 58s 357ms/step - loss: 0.1988 - acc: 0.9350 - top@1_acc: 0.9350 - top@5_acc: 0.9971 - val_loss: 0.2434 - val_acc: 0.9226 - val_top@1_acc: 0.9226 - val_top@5_acc: 0.9951\n",
      "Epoch 5/30\n",
      "160/160 [==============================] - 57s 357ms/step - loss: 0.1644 - acc: 0.9463 - top@1_acc: 0.9463 - top@5_acc: 0.9982 - val_loss: 0.2145 - val_acc: 0.9359 - val_top@1_acc: 0.9359 - val_top@5_acc: 0.9951\n",
      "Epoch 6/30\n",
      "160/160 [==============================] - 57s 356ms/step - loss: 0.1395 - acc: 0.9531 - top@1_acc: 0.9531 - top@5_acc: 0.9988 - val_loss: 0.2380 - val_acc: 0.9314 - val_top@1_acc: 0.9314 - val_top@5_acc: 0.9947\n",
      "Epoch 7/30\n",
      "160/160 [==============================] - 57s 355ms/step - loss: 0.1238 - acc: 0.9597 - top@1_acc: 0.9597 - top@5_acc: 0.9990 - val_loss: 0.2210 - val_acc: 0.9351 - val_top@1_acc: 0.9351 - val_top@5_acc: 0.9955\n",
      "Epoch 8/30\n",
      "160/160 [==============================] - 57s 356ms/step - loss: 0.1112 - acc: 0.9624 - top@1_acc: 0.9624 - top@5_acc: 0.9994 - val_loss: 0.2521 - val_acc: 0.9301 - val_top@1_acc: 0.9301 - val_top@5_acc: 0.9956\n",
      "Epoch 9/30\n",
      "160/160 [==============================] - 57s 355ms/step - loss: 0.0677 - acc: 0.9777 - top@1_acc: 0.9777 - top@5_acc: 0.9997 - val_loss: 0.1757 - val_acc: 0.9519 - val_top@1_acc: 0.9519 - val_top@5_acc: 0.9962\n",
      "Epoch 10/30\n",
      "160/160 [==============================] - 57s 355ms/step - loss: 0.0434 - acc: 0.9857 - top@1_acc: 0.9857 - top@5_acc: 0.9999 - val_loss: 0.1639 - val_acc: 0.9548 - val_top@1_acc: 0.9548 - val_top@5_acc: 0.9964\n",
      "Epoch 11/30\n",
      "160/160 [==============================] - 58s 359ms/step - loss: 0.0387 - acc: 0.9873 - top@1_acc: 0.9873 - top@5_acc: 0.9999 - val_loss: 0.1763 - val_acc: 0.9525 - val_top@1_acc: 0.9525 - val_top@5_acc: 0.9970\n",
      "Epoch 12/30\n",
      "160/160 [==============================] - 58s 359ms/step - loss: 0.0315 - acc: 0.9896 - top@1_acc: 0.9896 - top@5_acc: 1.0000 - val_loss: 0.1711 - val_acc: 0.9564 - val_top@1_acc: 0.9564 - val_top@5_acc: 0.9972\n",
      "Epoch 13/30\n",
      "160/160 [==============================] - 58s 358ms/step - loss: 0.0278 - acc: 0.9906 - top@1_acc: 0.9906 - top@5_acc: 0.9999 - val_loss: 0.1728 - val_acc: 0.9530 - val_top@1_acc: 0.9530 - val_top@5_acc: 0.9966\n",
      "Epoch 14/30\n",
      "160/160 [==============================] - 57s 357ms/step - loss: 0.0240 - acc: 0.9920 - top@1_acc: 0.9920 - top@5_acc: 1.0000 - val_loss: 0.1683 - val_acc: 0.9572 - val_top@1_acc: 0.9572 - val_top@5_acc: 0.9967\n",
      "Epoch 15/30\n",
      "160/160 [==============================] - 57s 356ms/step - loss: 0.0208 - acc: 0.9932 - top@1_acc: 0.9932 - top@5_acc: 0.9999 - val_loss: 0.1655 - val_acc: 0.9580 - val_top@1_acc: 0.9580 - val_top@5_acc: 0.9967\n",
      "Epoch 16/30\n",
      "160/160 [==============================] - 57s 355ms/step - loss: 0.0209 - acc: 0.9936 - top@1_acc: 0.9936 - top@5_acc: 1.0000 - val_loss: 0.1699 - val_acc: 0.9575 - val_top@1_acc: 0.9575 - val_top@5_acc: 0.9974\n",
      "Epoch 17/30\n",
      "160/160 [==============================] - 57s 356ms/step - loss: 0.0186 - acc: 0.9943 - top@1_acc: 0.9943 - top@5_acc: 1.0000 - val_loss: 0.1653 - val_acc: 0.9570 - val_top@1_acc: 0.9570 - val_top@5_acc: 0.9970\n",
      "Epoch 18/30\n",
      "160/160 [==============================] - 57s 356ms/step - loss: 0.0177 - acc: 0.9943 - top@1_acc: 0.9943 - top@5_acc: 0.9999 - val_loss: 0.1700 - val_acc: 0.9577 - val_top@1_acc: 0.9577 - val_top@5_acc: 0.9967\n",
      "Epoch 19/30\n",
      "160/160 [==============================] - 57s 356ms/step - loss: 0.0199 - acc: 0.9936 - top@1_acc: 0.9936 - top@5_acc: 0.9999 - val_loss: 0.1685 - val_acc: 0.9574 - val_top@1_acc: 0.9574 - val_top@5_acc: 0.9971\n",
      "Epoch 20/30\n",
      "160/160 [==============================] - 57s 357ms/step - loss: 0.0187 - acc: 0.9944 - top@1_acc: 0.9944 - top@5_acc: 1.0000 - val_loss: 0.1685 - val_acc: 0.9594 - val_top@1_acc: 0.9594 - val_top@5_acc: 0.9971\n",
      "Epoch 21/30\n",
      "160/160 [==============================] - 57s 357ms/step - loss: 0.0164 - acc: 0.9947 - top@1_acc: 0.9947 - top@5_acc: 1.0000 - val_loss: 0.1717 - val_acc: 0.9567 - val_top@1_acc: 0.9567 - val_top@5_acc: 0.9971\n",
      "Epoch 22/30\n",
      "160/160 [==============================] - 58s 358ms/step - loss: 0.0192 - acc: 0.9936 - top@1_acc: 0.9936 - top@5_acc: 1.0000 - val_loss: 0.1747 - val_acc: 0.9559 - val_top@1_acc: 0.9559 - val_top@5_acc: 0.9974\n",
      "Epoch 23/30\n",
      "160/160 [==============================] - 57s 357ms/step - loss: 0.0192 - acc: 0.9942 - top@1_acc: 0.9942 - top@5_acc: 1.0000 - val_loss: 0.1672 - val_acc: 0.9580 - val_top@1_acc: 0.9580 - val_top@5_acc: 0.9970\n",
      "Epoch 24/30\n",
      "160/160 [==============================] - 57s 356ms/step - loss: 0.0190 - acc: 0.9939 - top@1_acc: 0.9939 - top@5_acc: 0.9999 - val_loss: 0.1706 - val_acc: 0.9578 - val_top@1_acc: 0.9578 - val_top@5_acc: 0.9972\n",
      "Epoch 25/30\n",
      "160/160 [==============================] - 57s 356ms/step - loss: 0.0185 - acc: 0.9942 - top@1_acc: 0.9942 - top@5_acc: 1.0000 - val_loss: 0.1700 - val_acc: 0.9570 - val_top@1_acc: 0.9570 - val_top@5_acc: 0.9973\n",
      "Epoch 26/30\n",
      "160/160 [==============================] - 60s 370ms/step - loss: 0.0194 - acc: 0.9936 - top@1_acc: 0.9936 - top@5_acc: 1.0000 - val_loss: 0.1689 - val_acc: 0.9589 - val_top@1_acc: 0.9589 - val_top@5_acc: 0.9973\n",
      "Epoch 27/30\n",
      "160/160 [==============================] - 58s 364ms/step - loss: 0.0176 - acc: 0.9946 - top@1_acc: 0.9946 - top@5_acc: 1.0000 - val_loss: 0.1688 - val_acc: 0.9578 - val_top@1_acc: 0.9578 - val_top@5_acc: 0.9969\n",
      "Epoch 28/30\n",
      "160/160 [==============================] - 59s 370ms/step - loss: 0.0179 - acc: 0.9945 - top@1_acc: 0.9945 - top@5_acc: 1.0000 - val_loss: 0.1652 - val_acc: 0.9587 - val_top@1_acc: 0.9587 - val_top@5_acc: 0.9969\n",
      "Epoch 29/30\n",
      "160/160 [==============================] - 58s 360ms/step - loss: 0.0173 - acc: 0.9947 - top@1_acc: 0.9947 - top@5_acc: 1.0000 - val_loss: 0.1671 - val_acc: 0.9584 - val_top@1_acc: 0.9584 - val_top@5_acc: 0.9972\n",
      "Epoch 30/30\n",
      "160/160 [==============================] - 61s 379ms/step - loss: 0.0177 - acc: 0.9943 - top@1_acc: 0.9943 - top@5_acc: 1.0000 - val_loss: 0.1714 - val_acc: 0.9563 - val_top@1_acc: 0.9563 - val_top@5_acc: 0.9971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15828... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f256b8dffc3c41b3bfe57ec8a0fe448c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▅▆▆▇▇▇▇▇█████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>top@1_acc</td><td>▁▅▆▆▇▇▇▇▇█████████████████████</td></tr><tr><td>top@5_acc</td><td>▁▇▇███████████████████████████</td></tr><tr><td>val_acc</td><td>▁▄▅▅▆▆▆▆▇█▇█▇█████████████████</td></tr><tr><td>val_loss</td><td>█▄▄▃▂▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_top@1_acc</td><td>▁▄▅▅▆▆▆▆▇█▇█▇█████████████████</td></tr><tr><td>val_top@5_acc</td><td>▁▅▅▆▆▆▇▇▇▇██▇█████████████████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.99434</td></tr><tr><td>best_epoch</td><td>9</td></tr><tr><td>best_val_loss</td><td>0.16392</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>loss</td><td>0.01771</td></tr><tr><td>top@1_acc</td><td>0.99434</td></tr><tr><td>top@5_acc</td><td>0.99998</td></tr><tr><td>val_acc</td><td>0.9563</td></tr><tr><td>val_loss</td><td>0.17138</td></tr><tr><td>val_top@1_acc</td><td>0.9563</td></tr><tr><td>val_top@5_acc</td><td>0.99706</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">VYDM715W_0_train</strong>: <a href=\"https://wandb.ai/ayut/happywhale/runs/19qx9i6i\" target=\"_blank\">https://wandb.ai/ayut/happywhale/runs/19qx9i6i</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220209_213956-19qx9i6i/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ayut/happywhale/runs/1p4nqej2\" target=\"_blank\">VYDM715W_1_train</a></strong> to <a href=\"https://wandb.ai/ayut/happywhale\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "160/160 [==============================] - 70s 392ms/step - loss: 0.8385 - acc: 0.7518 - top@1_acc: 0.7518 - top@5_acc: 0.9385 - val_loss: 0.4025 - val_acc: 0.8692 - val_top@1_acc: 0.8692 - val_top@5_acc: 0.9887\n",
      "Epoch 2/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.3557 - acc: 0.8875 - top@1_acc: 0.8875 - top@5_acc: 0.9911 - val_loss: 0.3040 - val_acc: 0.9056 - val_top@1_acc: 0.9056 - val_top@5_acc: 0.9935\n",
      "Epoch 3/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.2567 - acc: 0.9175 - top@1_acc: 0.9175 - top@5_acc: 0.9953 - val_loss: 0.2520 - val_acc: 0.9181 - val_top@1_acc: 0.9181 - val_top@5_acc: 0.9942\n",
      "Epoch 4/30\n",
      "160/160 [==============================] - 58s 362ms/step - loss: 0.2020 - acc: 0.9339 - top@1_acc: 0.9339 - top@5_acc: 0.9972 - val_loss: 0.2147 - val_acc: 0.9330 - val_top@1_acc: 0.9330 - val_top@5_acc: 0.9966\n",
      "Epoch 5/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.1728 - acc: 0.9427 - top@1_acc: 0.9427 - top@5_acc: 0.9981 - val_loss: 0.2204 - val_acc: 0.9303 - val_top@1_acc: 0.9303 - val_top@5_acc: 0.9964\n",
      "Epoch 6/30\n",
      "160/160 [==============================] - 58s 361ms/step - loss: 0.1469 - acc: 0.9513 - top@1_acc: 0.9513 - top@5_acc: 0.9990 - val_loss: 0.2080 - val_acc: 0.9365 - val_top@1_acc: 0.9365 - val_top@5_acc: 0.9968\n",
      "Epoch 7/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.1336 - acc: 0.9566 - top@1_acc: 0.9566 - top@5_acc: 0.9986 - val_loss: 0.2007 - val_acc: 0.9388 - val_top@1_acc: 0.9388 - val_top@5_acc: 0.9967\n",
      "Epoch 8/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.1167 - acc: 0.9612 - top@1_acc: 0.9612 - top@5_acc: 0.9989 - val_loss: 0.1923 - val_acc: 0.9408 - val_top@1_acc: 0.9408 - val_top@5_acc: 0.9970\n",
      "Epoch 9/30\n",
      "160/160 [==============================] - 59s 364ms/step - loss: 0.1081 - acc: 0.9648 - top@1_acc: 0.9648 - top@5_acc: 0.9994 - val_loss: 0.2193 - val_acc: 0.9375 - val_top@1_acc: 0.9375 - val_top@5_acc: 0.9963\n",
      "Epoch 10/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0981 - acc: 0.9668 - top@1_acc: 0.9668 - top@5_acc: 0.9996 - val_loss: 0.2080 - val_acc: 0.9406 - val_top@1_acc: 0.9406 - val_top@5_acc: 0.9959\n",
      "Epoch 11/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0905 - acc: 0.9694 - top@1_acc: 0.9694 - top@5_acc: 0.9996 - val_loss: 0.1832 - val_acc: 0.9484 - val_top@1_acc: 0.9484 - val_top@5_acc: 0.9975\n",
      "Epoch 12/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.0848 - acc: 0.9709 - top@1_acc: 0.9709 - top@5_acc: 0.9997 - val_loss: 0.1954 - val_acc: 0.9411 - val_top@1_acc: 0.9411 - val_top@5_acc: 0.9974\n",
      "Epoch 13/30\n",
      "160/160 [==============================] - 59s 369ms/step - loss: 0.0765 - acc: 0.9747 - top@1_acc: 0.9747 - top@5_acc: 0.9997 - val_loss: 0.2053 - val_acc: 0.9430 - val_top@1_acc: 0.9430 - val_top@5_acc: 0.9963\n",
      "Epoch 14/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.0731 - acc: 0.9747 - top@1_acc: 0.9747 - top@5_acc: 0.9998 - val_loss: 0.1914 - val_acc: 0.9491 - val_top@1_acc: 0.9491 - val_top@5_acc: 0.9955\n",
      "Epoch 15/30\n",
      "160/160 [==============================] - 59s 364ms/step - loss: 0.0411 - acc: 0.9867 - top@1_acc: 0.9867 - top@5_acc: 0.9998 - val_loss: 0.1534 - val_acc: 0.9582 - val_top@1_acc: 0.9582 - val_top@5_acc: 0.9975\n",
      "Epoch 16/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.0244 - acc: 0.9918 - top@1_acc: 0.9918 - top@5_acc: 1.0000 - val_loss: 0.1539 - val_acc: 0.9569 - val_top@1_acc: 0.9569 - val_top@5_acc: 0.9976\n",
      "Epoch 17/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.0209 - acc: 0.9937 - top@1_acc: 0.9937 - top@5_acc: 1.0000 - val_loss: 0.1490 - val_acc: 0.9599 - val_top@1_acc: 0.9599 - val_top@5_acc: 0.9975\n",
      "Epoch 18/30\n",
      "160/160 [==============================] - 58s 361ms/step - loss: 0.0188 - acc: 0.9938 - top@1_acc: 0.9938 - top@5_acc: 1.0000 - val_loss: 0.1523 - val_acc: 0.9598 - val_top@1_acc: 0.9598 - val_top@5_acc: 0.9975\n",
      "Epoch 19/30\n",
      "160/160 [==============================] - 59s 364ms/step - loss: 0.0154 - acc: 0.9951 - top@1_acc: 0.9951 - top@5_acc: 1.0000 - val_loss: 0.1591 - val_acc: 0.9604 - val_top@1_acc: 0.9604 - val_top@5_acc: 0.9975\n",
      "Epoch 20/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.0153 - acc: 0.9952 - top@1_acc: 0.9952 - top@5_acc: 1.0000 - val_loss: 0.1548 - val_acc: 0.9614 - val_top@1_acc: 0.9614 - val_top@5_acc: 0.9982\n",
      "Epoch 21/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0139 - acc: 0.9961 - top@1_acc: 0.9961 - top@5_acc: 1.0000 - val_loss: 0.1527 - val_acc: 0.9607 - val_top@1_acc: 0.9607 - val_top@5_acc: 0.9979\n",
      "Epoch 22/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.0118 - acc: 0.9961 - top@1_acc: 0.9961 - top@5_acc: 1.0000 - val_loss: 0.1529 - val_acc: 0.9607 - val_top@1_acc: 0.9607 - val_top@5_acc: 0.9975\n",
      "Epoch 23/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.0113 - acc: 0.9967 - top@1_acc: 0.9967 - top@5_acc: 1.0000 - val_loss: 0.1594 - val_acc: 0.9604 - val_top@1_acc: 0.9604 - val_top@5_acc: 0.9974\n",
      "Epoch 24/30\n",
      "160/160 [==============================] - 59s 370ms/step - loss: 0.0098 - acc: 0.9969 - top@1_acc: 0.9969 - top@5_acc: 1.0000 - val_loss: 0.1549 - val_acc: 0.9618 - val_top@1_acc: 0.9618 - val_top@5_acc: 0.9980\n",
      "Epoch 25/30\n",
      "160/160 [==============================] - 60s 372ms/step - loss: 0.0097 - acc: 0.9969 - top@1_acc: 0.9969 - top@5_acc: 1.0000 - val_loss: 0.1503 - val_acc: 0.9618 - val_top@1_acc: 0.9618 - val_top@5_acc: 0.9981\n",
      "Epoch 26/30\n",
      "160/160 [==============================] - 59s 369ms/step - loss: 0.0094 - acc: 0.9969 - top@1_acc: 0.9969 - top@5_acc: 1.0000 - val_loss: 0.1548 - val_acc: 0.9606 - val_top@1_acc: 0.9606 - val_top@5_acc: 0.9979\n",
      "Epoch 27/30\n",
      "160/160 [==============================] - 60s 371ms/step - loss: 0.0099 - acc: 0.9972 - top@1_acc: 0.9972 - top@5_acc: 1.0000 - val_loss: 0.1549 - val_acc: 0.9618 - val_top@1_acc: 0.9618 - val_top@5_acc: 0.9976\n",
      "Epoch 28/30\n",
      "160/160 [==============================] - 59s 370ms/step - loss: 0.0094 - acc: 0.9971 - top@1_acc: 0.9971 - top@5_acc: 1.0000 - val_loss: 0.1569 - val_acc: 0.9610 - val_top@1_acc: 0.9610 - val_top@5_acc: 0.9979\n",
      "Epoch 29/30\n",
      "160/160 [==============================] - 60s 370ms/step - loss: 0.0099 - acc: 0.9972 - top@1_acc: 0.9972 - top@5_acc: 1.0000 - val_loss: 0.1509 - val_acc: 0.9621 - val_top@1_acc: 0.9621 - val_top@5_acc: 0.9976\n",
      "Epoch 30/30\n",
      "160/160 [==============================] - 60s 371ms/step - loss: 0.0096 - acc: 0.9972 - top@1_acc: 0.9972 - top@5_acc: 1.0000 - val_loss: 0.1542 - val_acc: 0.9614 - val_top@1_acc: 0.9614 - val_top@5_acc: 0.9977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17124... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b1b4d85bcc46059e2031b52c182021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>top@1_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>top@5_acc</td><td>▁▇▇███████████████████████████</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▆▆▆▆▆▇▆▇▇████████████████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▃▃▂▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_top@1_acc</td><td>▁▄▅▆▆▆▆▆▆▆▇▆▇▇████████████████</td></tr><tr><td>val_top@5_acc</td><td>▁▅▅▇▇▇▇▇▇▆▇▇▇▆▇█▇▇▇██▇▇███████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.99723</td></tr><tr><td>best_epoch</td><td>16</td></tr><tr><td>best_val_loss</td><td>0.14902</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>loss</td><td>0.00964</td></tr><tr><td>top@1_acc</td><td>0.99723</td></tr><tr><td>top@5_acc</td><td>1.0</td></tr><tr><td>val_acc</td><td>0.9614</td></tr><tr><td>val_loss</td><td>0.15417</td></tr><tr><td>val_top@1_acc</td><td>0.9614</td></tr><tr><td>val_top@5_acc</td><td>0.99775</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">VYDM715W_1_train</strong>: <a href=\"https://wandb.ai/ayut/happywhale/runs/1p4nqej2\" target=\"_blank\">https://wandb.ai/ayut/happywhale/runs/1p4nqej2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220209_221032-1p4nqej2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ayut/happywhale/runs/380kkxhn\" target=\"_blank\">VYDM715W_2_train</a></strong> to <a href=\"https://wandb.ai/ayut/happywhale\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "160/160 [==============================] - 72s 403ms/step - loss: 0.8251 - acc: 0.7559 - top@1_acc: 0.7559 - top@5_acc: 0.9405 - val_loss: 0.3666 - val_acc: 0.8839 - val_top@1_acc: 0.8839 - val_top@5_acc: 0.9903\n",
      "Epoch 2/30\n",
      "160/160 [==============================] - 60s 370ms/step - loss: 0.3449 - acc: 0.8915 - top@1_acc: 0.8915 - top@5_acc: 0.9916 - val_loss: 0.2743 - val_acc: 0.9116 - val_top@1_acc: 0.9116 - val_top@5_acc: 0.9938\n",
      "Epoch 3/30\n",
      "160/160 [==============================] - 60s 371ms/step - loss: 0.2502 - acc: 0.9189 - top@1_acc: 0.9189 - top@5_acc: 0.9957 - val_loss: 0.2371 - val_acc: 0.9267 - val_top@1_acc: 0.9267 - val_top@5_acc: 0.9946\n",
      "Epoch 4/30\n",
      "160/160 [==============================] - 60s 371ms/step - loss: 0.2044 - acc: 0.9331 - top@1_acc: 0.9331 - top@5_acc: 0.9970 - val_loss: 0.2194 - val_acc: 0.9312 - val_top@1_acc: 0.9312 - val_top@5_acc: 0.9946\n",
      "Epoch 5/30\n",
      "160/160 [==============================] - 60s 371ms/step - loss: 0.1644 - acc: 0.9460 - top@1_acc: 0.9460 - top@5_acc: 0.9983 - val_loss: 0.2280 - val_acc: 0.9317 - val_top@1_acc: 0.9317 - val_top@5_acc: 0.9951\n",
      "Epoch 6/30\n",
      "160/160 [==============================] - 60s 372ms/step - loss: 0.1506 - acc: 0.9504 - top@1_acc: 0.9504 - top@5_acc: 0.9987 - val_loss: 0.2251 - val_acc: 0.9334 - val_top@1_acc: 0.9334 - val_top@5_acc: 0.9953\n",
      "Epoch 7/30\n",
      "160/160 [==============================] - 60s 373ms/step - loss: 0.1312 - acc: 0.9559 - top@1_acc: 0.9559 - top@5_acc: 0.9992 - val_loss: 0.2159 - val_acc: 0.9350 - val_top@1_acc: 0.9350 - val_top@5_acc: 0.9958\n",
      "Epoch 8/30\n",
      "160/160 [==============================] - 59s 369ms/step - loss: 0.1140 - acc: 0.9609 - top@1_acc: 0.9609 - top@5_acc: 0.9993 - val_loss: 0.2110 - val_acc: 0.9350 - val_top@1_acc: 0.9350 - val_top@5_acc: 0.9968\n",
      "Epoch 9/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.1002 - acc: 0.9659 - top@1_acc: 0.9659 - top@5_acc: 0.9994 - val_loss: 0.2078 - val_acc: 0.9406 - val_top@1_acc: 0.9406 - val_top@5_acc: 0.9967\n",
      "Epoch 10/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.0964 - acc: 0.9672 - top@1_acc: 0.9672 - top@5_acc: 0.9997 - val_loss: 0.2145 - val_acc: 0.9387 - val_top@1_acc: 0.9387 - val_top@5_acc: 0.9971\n",
      "Epoch 11/30\n",
      "160/160 [==============================] - 59s 370ms/step - loss: 0.0927 - acc: 0.9688 - top@1_acc: 0.9688 - top@5_acc: 0.9995 - val_loss: 0.1975 - val_acc: 0.9413 - val_top@1_acc: 0.9413 - val_top@5_acc: 0.9970\n",
      "Epoch 12/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0814 - acc: 0.9735 - top@1_acc: 0.9735 - top@5_acc: 0.9996 - val_loss: 0.1965 - val_acc: 0.9443 - val_top@1_acc: 0.9443 - val_top@5_acc: 0.9981\n",
      "Epoch 13/30\n",
      "160/160 [==============================] - 58s 364ms/step - loss: 0.0766 - acc: 0.9751 - top@1_acc: 0.9751 - top@5_acc: 0.9997 - val_loss: 0.1806 - val_acc: 0.9484 - val_top@1_acc: 0.9484 - val_top@5_acc: 0.9974\n",
      "Epoch 14/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.0726 - acc: 0.9761 - top@1_acc: 0.9761 - top@5_acc: 0.9998 - val_loss: 0.2056 - val_acc: 0.9425 - val_top@1_acc: 0.9425 - val_top@5_acc: 0.9969\n",
      "Epoch 15/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0654 - acc: 0.9787 - top@1_acc: 0.9787 - top@5_acc: 0.9998 - val_loss: 0.1874 - val_acc: 0.9493 - val_top@1_acc: 0.9493 - val_top@5_acc: 0.9971\n",
      "Epoch 16/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0657 - acc: 0.9777 - top@1_acc: 0.9777 - top@5_acc: 0.9998 - val_loss: 0.2151 - val_acc: 0.9429 - val_top@1_acc: 0.9429 - val_top@5_acc: 0.9966\n",
      "Epoch 17/30\n",
      "160/160 [==============================] - 59s 364ms/step - loss: 0.0364 - acc: 0.9890 - top@1_acc: 0.9890 - top@5_acc: 1.0000 - val_loss: 0.1573 - val_acc: 0.9587 - val_top@1_acc: 0.9587 - val_top@5_acc: 0.9974\n",
      "Epoch 18/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.0217 - acc: 0.9928 - top@1_acc: 0.9928 - top@5_acc: 1.0000 - val_loss: 0.1534 - val_acc: 0.9602 - val_top@1_acc: 0.9602 - val_top@5_acc: 0.9979\n",
      "Epoch 19/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0162 - acc: 0.9953 - top@1_acc: 0.9953 - top@5_acc: 1.0000 - val_loss: 0.1571 - val_acc: 0.9604 - val_top@1_acc: 0.9604 - val_top@5_acc: 0.9979\n",
      "Epoch 20/30\n",
      "160/160 [==============================] - 59s 364ms/step - loss: 0.0166 - acc: 0.9950 - top@1_acc: 0.9950 - top@5_acc: 1.0000 - val_loss: 0.1632 - val_acc: 0.9591 - val_top@1_acc: 0.9591 - val_top@5_acc: 0.9977\n",
      "Epoch 21/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0149 - acc: 0.9956 - top@1_acc: 0.9956 - top@5_acc: 1.0000 - val_loss: 0.1632 - val_acc: 0.9621 - val_top@1_acc: 0.9621 - val_top@5_acc: 0.9976\n",
      "Epoch 22/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0115 - acc: 0.9965 - top@1_acc: 0.9965 - top@5_acc: 1.0000 - val_loss: 0.1546 - val_acc: 0.9618 - val_top@1_acc: 0.9618 - val_top@5_acc: 0.9979\n",
      "Epoch 23/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.0098 - acc: 0.9972 - top@1_acc: 0.9972 - top@5_acc: 1.0000 - val_loss: 0.1531 - val_acc: 0.9624 - val_top@1_acc: 0.9624 - val_top@5_acc: 0.9980\n",
      "Epoch 24/30\n",
      "160/160 [==============================] - 60s 373ms/step - loss: 0.0094 - acc: 0.9969 - top@1_acc: 0.9969 - top@5_acc: 1.0000 - val_loss: 0.1524 - val_acc: 0.9620 - val_top@1_acc: 0.9620 - val_top@5_acc: 0.9978\n",
      "Epoch 25/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.0097 - acc: 0.9969 - top@1_acc: 0.9969 - top@5_acc: 1.0000 - val_loss: 0.1554 - val_acc: 0.9629 - val_top@1_acc: 0.9629 - val_top@5_acc: 0.9979\n",
      "Epoch 26/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.0081 - acc: 0.9975 - top@1_acc: 0.9975 - top@5_acc: 1.0000 - val_loss: 0.1538 - val_acc: 0.9611 - val_top@1_acc: 0.9611 - val_top@5_acc: 0.9978\n",
      "Epoch 27/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0099 - acc: 0.9970 - top@1_acc: 0.9970 - top@5_acc: 1.0000 - val_loss: 0.1564 - val_acc: 0.9630 - val_top@1_acc: 0.9630 - val_top@5_acc: 0.9977\n",
      "Epoch 28/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0082 - acc: 0.9973 - top@1_acc: 0.9973 - top@5_acc: 1.0000 - val_loss: 0.1588 - val_acc: 0.9620 - val_top@1_acc: 0.9620 - val_top@5_acc: 0.9976\n",
      "Epoch 29/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.0092 - acc: 0.9972 - top@1_acc: 0.9972 - top@5_acc: 1.0000 - val_loss: 0.1566 - val_acc: 0.9633 - val_top@1_acc: 0.9633 - val_top@5_acc: 0.9980\n",
      "Epoch 30/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0081 - acc: 0.9975 - top@1_acc: 0.9975 - top@5_acc: 1.0000 - val_loss: 0.1564 - val_acc: 0.9629 - val_top@1_acc: 0.9629 - val_top@5_acc: 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 18416... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef81ef398f6c40149b8cfcdb203e0412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>top@1_acc</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>top@5_acc</td><td>▁▇▇███████████████████████████</td></tr><tr><td>val_acc</td><td>▁▃▅▅▅▅▆▆▆▆▆▆▇▆▇▆██████████████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▃▃▃▃▂▂▂▃▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_top@1_acc</td><td>▁▃▅▅▅▅▆▆▆▆▆▆▇▆▇▆██████████████</td></tr><tr><td>val_top@5_acc</td><td>▁▄▅▅▅▅▆▇▇▇▇█▇▇▇▇▇█████████████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.99745</td></tr><tr><td>best_epoch</td><td>23</td></tr><tr><td>best_val_loss</td><td>0.15241</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>loss</td><td>0.00807</td></tr><tr><td>top@1_acc</td><td>0.99745</td></tr><tr><td>top@5_acc</td><td>1.0</td></tr><tr><td>val_acc</td><td>0.96287</td></tr><tr><td>val_loss</td><td>0.15639</td></tr><tr><td>val_top@1_acc</td><td>0.96287</td></tr><tr><td>val_top@5_acc</td><td>0.99804</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">VYDM715W_2_train</strong>: <a href=\"https://wandb.ai/ayut/happywhale/runs/380kkxhn\" target=\"_blank\">https://wandb.ai/ayut/happywhale/runs/380kkxhn</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220209_224139-380kkxhn/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ayut/happywhale/runs/2xdvz1xl\" target=\"_blank\">VYDM715W_3_train</a></strong> to <a href=\"https://wandb.ai/ayut/happywhale\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "160/160 [==============================] - 72s 401ms/step - loss: 0.8170 - acc: 0.7582 - top@1_acc: 0.7582 - top@5_acc: 0.9390 - val_loss: 0.4181 - val_acc: 0.8678 - val_top@1_acc: 0.8678 - val_top@5_acc: 0.9871\n",
      "Epoch 2/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.3535 - acc: 0.8878 - top@1_acc: 0.8878 - top@5_acc: 0.9903 - val_loss: 0.3099 - val_acc: 0.9004 - val_top@1_acc: 0.9004 - val_top@5_acc: 0.9922\n",
      "Epoch 3/30\n",
      "160/160 [==============================] - 59s 369ms/step - loss: 0.2571 - acc: 0.9181 - top@1_acc: 0.9181 - top@5_acc: 0.9951 - val_loss: 0.2691 - val_acc: 0.9167 - val_top@1_acc: 0.9167 - val_top@5_acc: 0.9939\n",
      "Epoch 4/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.2108 - acc: 0.9309 - top@1_acc: 0.9309 - top@5_acc: 0.9968 - val_loss: 0.2608 - val_acc: 0.9162 - val_top@1_acc: 0.9162 - val_top@5_acc: 0.9947\n",
      "Epoch 5/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.1679 - acc: 0.9455 - top@1_acc: 0.9455 - top@5_acc: 0.9979 - val_loss: 0.2344 - val_acc: 0.9278 - val_top@1_acc: 0.9278 - val_top@5_acc: 0.9958\n",
      "Epoch 6/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.1550 - acc: 0.9494 - top@1_acc: 0.9494 - top@5_acc: 0.9984 - val_loss: 0.2249 - val_acc: 0.9317 - val_top@1_acc: 0.9317 - val_top@5_acc: 0.9950\n",
      "Epoch 7/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.1308 - acc: 0.9574 - top@1_acc: 0.9574 - top@5_acc: 0.9990 - val_loss: 0.2179 - val_acc: 0.9343 - val_top@1_acc: 0.9343 - val_top@5_acc: 0.9961\n",
      "Epoch 8/30\n",
      "160/160 [==============================] - 59s 369ms/step - loss: 0.1179 - acc: 0.9617 - top@1_acc: 0.9617 - top@5_acc: 0.9991 - val_loss: 0.2048 - val_acc: 0.9393 - val_top@1_acc: 0.9393 - val_top@5_acc: 0.9960\n",
      "Epoch 9/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.1091 - acc: 0.9624 - top@1_acc: 0.9624 - top@5_acc: 0.9992 - val_loss: 0.2015 - val_acc: 0.9408 - val_top@1_acc: 0.9408 - val_top@5_acc: 0.9965\n",
      "Epoch 10/30\n",
      "160/160 [==============================] - 59s 369ms/step - loss: 0.1016 - acc: 0.9655 - top@1_acc: 0.9655 - top@5_acc: 0.9995 - val_loss: 0.2133 - val_acc: 0.9404 - val_top@1_acc: 0.9404 - val_top@5_acc: 0.9958\n",
      "Epoch 11/30\n",
      "160/160 [==============================] - 59s 369ms/step - loss: 0.0897 - acc: 0.9698 - top@1_acc: 0.9698 - top@5_acc: 0.9995 - val_loss: 0.2091 - val_acc: 0.9402 - val_top@1_acc: 0.9402 - val_top@5_acc: 0.9963\n",
      "Epoch 12/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0839 - acc: 0.9727 - top@1_acc: 0.9727 - top@5_acc: 0.9997 - val_loss: 0.1899 - val_acc: 0.9445 - val_top@1_acc: 0.9445 - val_top@5_acc: 0.9968\n",
      "Epoch 13/30\n",
      "160/160 [==============================] - 59s 369ms/step - loss: 0.0779 - acc: 0.9738 - top@1_acc: 0.9738 - top@5_acc: 0.9997 - val_loss: 0.2176 - val_acc: 0.9372 - val_top@1_acc: 0.9372 - val_top@5_acc: 0.9964\n",
      "Epoch 14/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0761 - acc: 0.9743 - top@1_acc: 0.9743 - top@5_acc: 0.9999 - val_loss: 0.1924 - val_acc: 0.9475 - val_top@1_acc: 0.9475 - val_top@5_acc: 0.9967\n",
      "Epoch 15/30\n",
      "160/160 [==============================] - 59s 364ms/step - loss: 0.0737 - acc: 0.9752 - top@1_acc: 0.9752 - top@5_acc: 0.9998 - val_loss: 0.1990 - val_acc: 0.9446 - val_top@1_acc: 0.9446 - val_top@5_acc: 0.9961\n",
      "Epoch 16/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0384 - acc: 0.9872 - top@1_acc: 0.9872 - top@5_acc: 0.9998 - val_loss: 0.1633 - val_acc: 0.9561 - val_top@1_acc: 0.9561 - val_top@5_acc: 0.9976\n",
      "Epoch 17/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0244 - acc: 0.9924 - top@1_acc: 0.9924 - top@5_acc: 1.0000 - val_loss: 0.1552 - val_acc: 0.9594 - val_top@1_acc: 0.9594 - val_top@5_acc: 0.9970\n",
      "Epoch 18/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0209 - acc: 0.9928 - top@1_acc: 0.9928 - top@5_acc: 1.0000 - val_loss: 0.1566 - val_acc: 0.9607 - val_top@1_acc: 0.9607 - val_top@5_acc: 0.9979\n",
      "Epoch 19/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0157 - acc: 0.9953 - top@1_acc: 0.9953 - top@5_acc: 1.0000 - val_loss: 0.1576 - val_acc: 0.9590 - val_top@1_acc: 0.9590 - val_top@5_acc: 0.9976\n",
      "Epoch 20/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0158 - acc: 0.9953 - top@1_acc: 0.9953 - top@5_acc: 1.0000 - val_loss: 0.1638 - val_acc: 0.9587 - val_top@1_acc: 0.9587 - val_top@5_acc: 0.9974\n",
      "Epoch 21/30\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 0.0134 - acc: 0.9960 - top@1_acc: 0.9960 - top@5_acc: 1.0000 - val_loss: 0.1551 - val_acc: 0.9594 - val_top@1_acc: 0.9594 - val_top@5_acc: 0.9979\n",
      "Epoch 22/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0138 - acc: 0.9957 - top@1_acc: 0.9957 - top@5_acc: 1.0000 - val_loss: 0.1580 - val_acc: 0.9618 - val_top@1_acc: 0.9618 - val_top@5_acc: 0.9979\n",
      "Epoch 23/30\n",
      "160/160 [==============================] - 60s 376ms/step - loss: 0.0106 - acc: 0.9967 - top@1_acc: 0.9967 - top@5_acc: 1.0000 - val_loss: 0.1554 - val_acc: 0.9603 - val_top@1_acc: 0.9603 - val_top@5_acc: 0.9977\n",
      "Epoch 24/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0116 - acc: 0.9965 - top@1_acc: 0.9965 - top@5_acc: 1.0000 - val_loss: 0.1553 - val_acc: 0.9615 - val_top@1_acc: 0.9615 - val_top@5_acc: 0.9975\n",
      "Epoch 25/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0103 - acc: 0.9970 - top@1_acc: 0.9970 - top@5_acc: 1.0000 - val_loss: 0.1552 - val_acc: 0.9619 - val_top@1_acc: 0.9619 - val_top@5_acc: 0.9975\n",
      "Epoch 26/30\n",
      "160/160 [==============================] - 60s 376ms/step - loss: 0.0112 - acc: 0.9965 - top@1_acc: 0.9965 - top@5_acc: 1.0000 - val_loss: 0.1545 - val_acc: 0.9617 - val_top@1_acc: 0.9617 - val_top@5_acc: 0.9976\n",
      "Epoch 27/30\n",
      "160/160 [==============================] - 58s 364ms/step - loss: 0.0102 - acc: 0.9969 - top@1_acc: 0.9969 - top@5_acc: 1.0000 - val_loss: 0.1549 - val_acc: 0.9608 - val_top@1_acc: 0.9608 - val_top@5_acc: 0.9979\n",
      "Epoch 28/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.0097 - acc: 0.9967 - top@1_acc: 0.9967 - top@5_acc: 1.0000 - val_loss: 0.1546 - val_acc: 0.9614 - val_top@1_acc: 0.9614 - val_top@5_acc: 0.9981\n",
      "Epoch 29/30\n",
      "160/160 [==============================] - 60s 376ms/step - loss: 0.0100 - acc: 0.9969 - top@1_acc: 0.9969 - top@5_acc: 1.0000 - val_loss: 0.1581 - val_acc: 0.9602 - val_top@1_acc: 0.9602 - val_top@5_acc: 0.9974\n",
      "Epoch 30/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0093 - acc: 0.9973 - top@1_acc: 0.9973 - top@5_acc: 1.0000 - val_loss: 0.1536 - val_acc: 0.9620 - val_top@1_acc: 0.9620 - val_top@5_acc: 0.9977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 19713... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d7a83986bd462ab6c9d1a4be310893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>top@1_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>top@5_acc</td><td>▁▇▇███████████████████████████</td></tr><tr><td>val_acc</td><td>▁▃▅▅▅▆▆▆▆▆▆▇▆▇▇███████████████</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▃▃▂▂▃▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_top@1_acc</td><td>▁▃▅▅▅▆▆▆▆▆▆▇▆▇▇███████████████</td></tr><tr><td>val_top@5_acc</td><td>▁▄▅▆▇▆▇▇▇▇▇▇▇▇▇█▇█████████████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.99728</td></tr><tr><td>best_epoch</td><td>29</td></tr><tr><td>best_val_loss</td><td>0.15359</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>loss</td><td>0.00935</td></tr><tr><td>top@1_acc</td><td>0.99728</td></tr><tr><td>top@5_acc</td><td>1.0</td></tr><tr><td>val_acc</td><td>0.96198</td></tr><tr><td>val_loss</td><td>0.15359</td></tr><tr><td>val_top@1_acc</td><td>0.96198</td></tr><tr><td>val_top@5_acc</td><td>0.99775</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">VYDM715W_3_train</strong>: <a href=\"https://wandb.ai/ayut/happywhale/runs/2xdvz1xl\" target=\"_blank\">https://wandb.ai/ayut/happywhale/runs/2xdvz1xl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220209_231251-2xdvz1xl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ayut/happywhale/runs/17ryevca\" target=\"_blank\">VYDM715W_4_train</a></strong> to <a href=\"https://wandb.ai/ayut/happywhale\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "160/160 [==============================] - 75s 420ms/step - loss: 0.8142 - acc: 0.7612 - top@1_acc: 0.7612 - top@5_acc: 0.9412 - val_loss: 0.4070 - val_acc: 0.8696 - val_top@1_acc: 0.8696 - val_top@5_acc: 0.9880\n",
      "Epoch 2/30\n",
      "160/160 [==============================] - 59s 369ms/step - loss: 0.3474 - acc: 0.8900 - top@1_acc: 0.8900 - top@5_acc: 0.9917 - val_loss: 0.3059 - val_acc: 0.8999 - val_top@1_acc: 0.8999 - val_top@5_acc: 0.9928\n",
      "Epoch 3/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.2562 - acc: 0.9170 - top@1_acc: 0.9170 - top@5_acc: 0.9951 - val_loss: 0.2796 - val_acc: 0.9108 - val_top@1_acc: 0.9108 - val_top@5_acc: 0.9941\n",
      "Epoch 4/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.1981 - acc: 0.9359 - top@1_acc: 0.9359 - top@5_acc: 0.9973 - val_loss: 0.2729 - val_acc: 0.9157 - val_top@1_acc: 0.9157 - val_top@5_acc: 0.9946\n",
      "Epoch 5/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.1704 - acc: 0.9447 - top@1_acc: 0.9447 - top@5_acc: 0.9981 - val_loss: 0.2322 - val_acc: 0.9259 - val_top@1_acc: 0.9259 - val_top@5_acc: 0.9962\n",
      "Epoch 6/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.1481 - acc: 0.9511 - top@1_acc: 0.9511 - top@5_acc: 0.9987 - val_loss: 0.2412 - val_acc: 0.9265 - val_top@1_acc: 0.9265 - val_top@5_acc: 0.9947\n",
      "Epoch 7/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.1254 - acc: 0.9576 - top@1_acc: 0.9576 - top@5_acc: 0.9990 - val_loss: 0.2421 - val_acc: 0.9278 - val_top@1_acc: 0.9278 - val_top@5_acc: 0.9957\n",
      "Epoch 8/30\n",
      "160/160 [==============================] - 60s 373ms/step - loss: 0.1126 - acc: 0.9615 - top@1_acc: 0.9615 - top@5_acc: 0.9993 - val_loss: 0.2301 - val_acc: 0.9324 - val_top@1_acc: 0.9324 - val_top@5_acc: 0.9955\n",
      "Epoch 9/30\n",
      "160/160 [==============================] - 59s 364ms/step - loss: 0.1059 - acc: 0.9655 - top@1_acc: 0.9655 - top@5_acc: 0.9993 - val_loss: 0.2299 - val_acc: 0.9349 - val_top@1_acc: 0.9349 - val_top@5_acc: 0.9963\n",
      "Epoch 10/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.0941 - acc: 0.9686 - top@1_acc: 0.9686 - top@5_acc: 0.9997 - val_loss: 0.2186 - val_acc: 0.9353 - val_top@1_acc: 0.9353 - val_top@5_acc: 0.9955\n",
      "Epoch 11/30\n",
      "160/160 [==============================] - 60s 371ms/step - loss: 0.0900 - acc: 0.9698 - top@1_acc: 0.9698 - top@5_acc: 0.9994 - val_loss: 0.2268 - val_acc: 0.9361 - val_top@1_acc: 0.9361 - val_top@5_acc: 0.9953\n",
      "Epoch 12/30\n",
      "160/160 [==============================] - 58s 363ms/step - loss: 0.0849 - acc: 0.9724 - top@1_acc: 0.9724 - top@5_acc: 0.9998 - val_loss: 0.2430 - val_acc: 0.9361 - val_top@1_acc: 0.9361 - val_top@5_acc: 0.9956\n",
      "Epoch 13/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0758 - acc: 0.9747 - top@1_acc: 0.9747 - top@5_acc: 0.9998 - val_loss: 0.2256 - val_acc: 0.9379 - val_top@1_acc: 0.9379 - val_top@5_acc: 0.9961\n",
      "Epoch 14/30\n",
      "160/160 [==============================] - 60s 371ms/step - loss: 0.0465 - acc: 0.9844 - top@1_acc: 0.9844 - top@5_acc: 0.9999 - val_loss: 0.1802 - val_acc: 0.9500 - val_top@1_acc: 0.9500 - val_top@5_acc: 0.9974\n",
      "Epoch 15/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0282 - acc: 0.9908 - top@1_acc: 0.9908 - top@5_acc: 1.0000 - val_loss: 0.1725 - val_acc: 0.9525 - val_top@1_acc: 0.9525 - val_top@5_acc: 0.9976\n",
      "Epoch 16/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0236 - acc: 0.9926 - top@1_acc: 0.9926 - top@5_acc: 1.0000 - val_loss: 0.1827 - val_acc: 0.9514 - val_top@1_acc: 0.9514 - val_top@5_acc: 0.9971\n",
      "Epoch 17/30\n",
      "160/160 [==============================] - 60s 371ms/step - loss: 0.0184 - acc: 0.9940 - top@1_acc: 0.9940 - top@5_acc: 0.9999 - val_loss: 0.1848 - val_acc: 0.9535 - val_top@1_acc: 0.9535 - val_top@5_acc: 0.9974\n",
      "Epoch 18/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.0167 - acc: 0.9945 - top@1_acc: 0.9945 - top@5_acc: 1.0000 - val_loss: 0.1803 - val_acc: 0.9539 - val_top@1_acc: 0.9539 - val_top@5_acc: 0.9972\n",
      "Epoch 19/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0140 - acc: 0.9956 - top@1_acc: 0.9956 - top@5_acc: 1.0000 - val_loss: 0.1747 - val_acc: 0.9550 - val_top@1_acc: 0.9550 - val_top@5_acc: 0.9970\n",
      "Epoch 20/30\n",
      "160/160 [==============================] - 60s 375ms/step - loss: 0.0133 - acc: 0.9959 - top@1_acc: 0.9959 - top@5_acc: 1.0000 - val_loss: 0.1727 - val_acc: 0.9562 - val_top@1_acc: 0.9562 - val_top@5_acc: 0.9976\n",
      "Epoch 21/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.0139 - acc: 0.9953 - top@1_acc: 0.9953 - top@5_acc: 1.0000 - val_loss: 0.1747 - val_acc: 0.9556 - val_top@1_acc: 0.9556 - val_top@5_acc: 0.9971\n",
      "Epoch 22/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.0120 - acc: 0.9962 - top@1_acc: 0.9962 - top@5_acc: 1.0000 - val_loss: 0.1774 - val_acc: 0.9562 - val_top@1_acc: 0.9562 - val_top@5_acc: 0.9970\n",
      "Epoch 23/30\n",
      "160/160 [==============================] - 59s 367ms/step - loss: 0.0118 - acc: 0.9963 - top@1_acc: 0.9963 - top@5_acc: 1.0000 - val_loss: 0.1797 - val_acc: 0.9577 - val_top@1_acc: 0.9577 - val_top@5_acc: 0.9972\n",
      "Epoch 24/30\n",
      "160/160 [==============================] - 59s 365ms/step - loss: 0.0117 - acc: 0.9964 - top@1_acc: 0.9964 - top@5_acc: 1.0000 - val_loss: 0.1776 - val_acc: 0.9562 - val_top@1_acc: 0.9562 - val_top@5_acc: 0.9975\n",
      "Epoch 25/30\n",
      "160/160 [==============================] - 58s 364ms/step - loss: 0.0113 - acc: 0.9965 - top@1_acc: 0.9965 - top@5_acc: 1.0000 - val_loss: 0.1743 - val_acc: 0.9555 - val_top@1_acc: 0.9555 - val_top@5_acc: 0.9974\n",
      "Epoch 26/30\n",
      "160/160 [==============================] - 59s 366ms/step - loss: 0.0126 - acc: 0.9960 - top@1_acc: 0.9960 - top@5_acc: 1.0000 - val_loss: 0.1715 - val_acc: 0.9574 - val_top@1_acc: 0.9574 - val_top@5_acc: 0.9974\n",
      "Epoch 27/30\n",
      "160/160 [==============================] - 58s 361ms/step - loss: 0.0127 - acc: 0.9960 - top@1_acc: 0.9960 - top@5_acc: 1.0000 - val_loss: 0.1775 - val_acc: 0.9554 - val_top@1_acc: 0.9554 - val_top@5_acc: 0.9975\n",
      "Epoch 28/30\n",
      "160/160 [==============================] - 58s 362ms/step - loss: 0.0117 - acc: 0.9966 - top@1_acc: 0.9966 - top@5_acc: 1.0000 - val_loss: 0.1745 - val_acc: 0.9567 - val_top@1_acc: 0.9567 - val_top@5_acc: 0.9975\n",
      "Epoch 29/30\n",
      "160/160 [==============================] - 58s 362ms/step - loss: 0.0098 - acc: 0.9971 - top@1_acc: 0.9971 - top@5_acc: 1.0000 - val_loss: 0.1733 - val_acc: 0.9578 - val_top@1_acc: 0.9578 - val_top@5_acc: 0.9972\n",
      "Epoch 30/30\n",
      "160/160 [==============================] - 58s 359ms/step - loss: 0.0128 - acc: 0.9958 - top@1_acc: 0.9958 - top@5_acc: 1.0000 - val_loss: 0.1730 - val_acc: 0.9558 - val_top@1_acc: 0.9558 - val_top@5_acc: 0.9971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21012... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6077e212dbd24793ae9f7fe8ee24bd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>top@1_acc</td><td>▁▅▆▆▆▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>top@5_acc</td><td>▁▇▇███████████████████████████</td></tr><tr><td>val_acc</td><td>▁▃▄▅▅▆▆▆▆▆▆▆▆▇█▇██████████████</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▃▃▃▃▂▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_top@1_acc</td><td>▁▃▄▅▅▆▆▆▆▆▆▆▆▇█▇██████████████</td></tr><tr><td>val_top@5_acc</td><td>▁▅▅▆▇▆▇▆▇▆▆▇▇█████████████████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.99576</td></tr><tr><td>best_epoch</td><td>25</td></tr><tr><td>best_val_loss</td><td>0.17153</td></tr><tr><td>epoch</td><td>29</td></tr><tr><td>loss</td><td>0.0128</td></tr><tr><td>top@1_acc</td><td>0.99576</td></tr><tr><td>top@5_acc</td><td>1.0</td></tr><tr><td>val_acc</td><td>0.95581</td></tr><tr><td>val_loss</td><td>0.17298</td></tr><tr><td>val_top@1_acc</td><td>0.95581</td></tr><tr><td>val_top@5_acc</td><td>0.99706</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">VYDM715W_4_train</strong>: <a href=\"https://wandb.ai/ayut/happywhale/runs/17ryevca\" target=\"_blank\">https://wandb.ai/ayut/happywhale/runs/17ryevca</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220209_234425-17ryevca/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    args.epochs = 10\n",
    "    args.num_folds = 1\n",
    "\n",
    "for fold in range(args.num_folds):\n",
    "    # Get dataloaders\n",
    "    train_df = df[df.fold != fold]\n",
    "    valid_df = df[df.fold == fold]\n",
    "\n",
    "    dataset = GetDataloader(args)\n",
    "    trainloader = dataset.dataloader(train_df)\n",
    "    validloader = dataset.dataloader(valid_df, data_type='valid')\n",
    "    \n",
    "    # Initialize model\n",
    "    tf.keras.backend.clear_session()\n",
    "    if args.use_arcface:\n",
    "        get_model = ArcFaceSupervisedModel(args)\n",
    "    else:\n",
    "        get_model = SimpleSupervisedtModel(args)\n",
    "        \n",
    "    model = get_model.get_efficientnet()\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = 'adam'\n",
    "    if args.use_arcface:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'categorical_crossentropy'\n",
    "        \n",
    "    model.compile(optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=['acc',\n",
    "                           tf.keras.metrics.TopKCategoricalAccuracy(1, name='top@1_acc'),\n",
    "                           tf.keras.metrics.TopKCategoricalAccuracy(5, name='top@5_acc')])\n",
    "\n",
    "    # Initialize W&B run\n",
    "    run = wandb.init(project='happywhale',\n",
    "                     config=vars(args),\n",
    "                     group=f'effnetb0-{args.exp_id}',\n",
    "                     job_type='train',\n",
    "                     name=f'{args.exp_id}_{fold}_train')\n",
    "\n",
    "    # Train\n",
    "    model.fit(trainloader,\n",
    "              epochs=args.epochs,\n",
    "              validation_data=validloader,\n",
    "              callbacks=[WandbCallback(save_model=False),\n",
    "                         callbacks.get_reduce_lr_on_plateau()])\n",
    "    \n",
    "    # Save the model\n",
    "    os.makedirs(f'{args.model_save_path}/{args.exp_id}', exist_ok=True)\n",
    "    model.save(f'{args.model_save_path}/{args.exp_id}/model_{fold}')\n",
    "    \n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model(f'{args.model_save_path}/{args.exp_id}/model_{fold}')\n",
    "    \n",
    "    # Evaluate and prepare oof \n",
    "    preds = model.predict(validloader)\n",
    "    df.loc[list(df[df.fold == fold].index), 'preds'] = np.argmax(preds, axis=1)\n",
    "    \n",
    "    # Get Embedding and save it as npz files along with validation index\n",
    "    feature_extractor = get_feature_extractor(model)\n",
    "    embedding = feature_extractor.predict(validloader)\n",
    "\n",
    "    os.makedirs(f'{args.embedding_save_path}/{args.exp_id}', exist_ok=True)\n",
    "    np.savez(f'{args.embedding_save_path}/{args.exp_id}/embedding_{fold}.npz',\n",
    "             embedding=embedding,\n",
    "             index=np.array(valid_df.index))\n",
    "    \n",
    "    del trainloader, validloader, model, feature_extractor, embedding\n",
    "    _ = gc.collect()\n",
    "\n",
    "    # Close W&B run\n",
    "    run.finish()\n",
    "    \n",
    "df[['image', 'individual_id', 'target', 'preds']].to_csv('../oof.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a529625a-0337-49af-bacc-cc6a4e56393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_df = pd.read_csv('../oof.csv')\n",
    "oof_df_copy = oof_df.copy()\n",
    "\n",
    "def correct_preds(row):\n",
    "    return int(row.preds)\n",
    "\n",
    "oof_df_copy['preds'] = oof_df_copy.apply(lambda row: correct_preds(row), axis=1)\n",
    "\n",
    "metric = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "metric.update_state(oof_df_copy.target.values.reshape(-1,1), oof_df_copy.preds.values.reshape(-1,1))\n",
    "print(f'CV Score: {metric.result().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d860996-1694-4d01-b706-ad335a1ba235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51027</th>\n",
       "      <td>fff603f5af8614.jpg</td>\n",
       "      <td>fin_whale</td>\n",
       "      <td>40fe65946167</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51028</th>\n",
       "      <td>fff639a7a78b3f.jpg</td>\n",
       "      <td>beluga</td>\n",
       "      <td>5ac053677ed1</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51030</th>\n",
       "      <td>fff94675cc1aef.jpg</td>\n",
       "      <td>blue_whale</td>\n",
       "      <td>5401612696b9</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51031</th>\n",
       "      <td>fffbc5dd642d8c.jpg</td>\n",
       "      <td>beluga</td>\n",
       "      <td>4000b3d7c24e</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51032</th>\n",
       "      <td>fffdcd42312777.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4ddb2eeb5efb</td>\n",
       "      <td>../128x128/train_images-128-128/train_images-1...</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47717 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image             species individual_id  \\\n",
       "0      00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9   \n",
       "1      000562241d384d.jpg      humpback_whale  1a71fbb72250   \n",
       "2      0007c33415ce37.jpg  false_killer_whale  60008f293a2b   \n",
       "3      0007d9bca26a99.jpg  bottlenose_dolphin  4b00fe572063   \n",
       "4      00087baf5cef7a.jpg      humpback_whale  8e5253662392   \n",
       "...                   ...                 ...           ...   \n",
       "51027  fff603f5af8614.jpg           fin_whale  40fe65946167   \n",
       "51028  fff639a7a78b3f.jpg              beluga  5ac053677ed1   \n",
       "51030  fff94675cc1aef.jpg          blue_whale  5401612696b9   \n",
       "51031  fffbc5dd642d8c.jpg              beluga  4000b3d7c24e   \n",
       "51032  fffdcd42312777.jpg  bottlenose_dolphin  4ddb2eeb5efb   \n",
       "\n",
       "                                                img_path  target  fold  preds  \n",
       "0      ../128x128/train_images-128-128/train_images-1...       0   2.0      0  \n",
       "1      ../128x128/train_images-128-128/train_images-1...       1   3.0      1  \n",
       "2      ../128x128/train_images-128-128/train_images-1...       2   2.0      2  \n",
       "3      ../128x128/train_images-128-128/train_images-1...       3   2.0      3  \n",
       "4      ../128x128/train_images-128-128/train_images-1...       1   4.0      1  \n",
       "...                                                  ...     ...   ...    ...  \n",
       "51027  ../128x128/train_images-128-128/train_images-1...       6   3.0      6  \n",
       "51028  ../128x128/train_images-128-128/train_images-1...       4   4.0      4  \n",
       "51030  ../128x128/train_images-128-128/train_images-1...       7   3.0      7  \n",
       "51031  ../128x128/train_images-128-128/train_images-1...       4   0.0      4  \n",
       "51032  ../128x128/train_images-128-128/train_images-1...       3   2.0      3  \n",
       "\n",
       "[47717 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_df_copy[oof_df_copy.target == oof_df_copy.preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c29449-880d-488e-b79b-10dd08a8b67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m73"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
